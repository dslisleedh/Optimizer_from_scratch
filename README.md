# Optimizer_from_scratch

 Implement optimizers and visualize how gradient descent is working.
 
# TODO:
 - [X] Implement optimizers  
   - [X] SGD  
   - [X] Momentum SGD  
   - [X] Nesterov Momentum SGD
   - [X] Adagrad
   - [X] RMSProp
   - [X] Adam
   - [X] Adabelief
  
 - [ ] Visualize gradient descent
