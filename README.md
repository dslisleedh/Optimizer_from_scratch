# Optimizer_from_scratch

 Implement optimizers and visualize how gradient descent is working.
 
# TODO:
 - [ ] Implement optimizers  
   - [ ] SGD  
   - [ ] Momentum SGD  
   - [ ] Nesterov Momentum SGD
   - [ ] Adagrad
   - [ ] RMSProp
   - [ ] Adam
   - [ ] Adabelief
  
 - [ ] Visualize gradient descent
